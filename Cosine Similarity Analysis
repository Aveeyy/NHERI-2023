# -*- coding: utf-8 -*-
"""Avey Copy of Branch_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiRmSupkD2C0lhRcl4_yT1bGOLCiuO84
"""

!pip install openai
!pip install markdown2
!pip install transformers

COMPLETIONS_MODEL = "text-davinci-003"
EMBEDDING_MODEL = "text-embedding-ada-002"

import numpy as np
import openai
import os
import pandas as pd
import pickle
import pyarrow

import os
openai.api_key = os.getenv("OPENAI_API_KEY")
assert openai.api_key, "Missing OPENAI_API_KEY env var"

from google.colab import drive
drive.mount('/content/drive')

directory = "/content/drive/MyDrive/Technical Documentation "
for i in os.listdir(directory):
  print(i)

import markdown2
from bs4 import BeautifulSoup
from transformers import GPT2TokenizerFast

import numpy as np
from nltk.tokenize import sent_tokenize

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

data = []

#creating empty data frame
#df_f = pd.DataFrame()
df_f = pd.DataFrame(data, columns=["file", "heading", "content", "tokens"])
#df_f = df_final.reset_index().drop('index',axis=1) # reset index

#iterating through each file to tokenize
for i in os.listdir(directory):
    path = os.path.join(directory, i)
    with open(path, "r", encoding="utf-8") as file:
        content = file.read()

    file_no_md = "tapipy/" + i.replace(".md", "")

    # markdown -> html -> soup
    html = markdown2.markdown(content)
    soup = BeautifulSoup(html, "html.parser")

    headings, paragraphs = [], []
    rows = []  # per-file rows

    def count_tokens(text: str) -> int:
        """Count GPT-2 tokens (approx) for budgeting."""
        return len(tokenizer.encode(text))

    def break_text(text: str, max_words: int = 300):
        """Break text into chunks of at most max_words words."""
        words = text.split()
        return [
            " ".join(words[j:j + max_words])
            for j in range(0, len(words), max_words)
        ]

    for tag in soup.descendants:
        if tag.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
            # flush previous section (heading + paragraphs) when a new heading starts
            if headings and paragraphs:
                hdgs = " ".join(headings)
                para = " ".join(paragraphs)

                if count_tokens(para) > 1024:
                    for chunk in break_text(para, max_words=300):
                        rows.append([file_no_md, hdgs, chunk, count_tokens(chunk)])
                else:
                    rows.append([file_no_md, hdgs, para, count_tokens(para)])

                headings, paragraphs = [], []

            headings.append(tag.text)

        elif tag.name == "p":
            paragraphs.append(tag.text)

    
    if headings and paragraphs:
        hdgs = " ".join(headings)
        para = " ".join(paragraphs)

        if count_tokens(para) > 1024:
            for chunk in break_text(para, max_words=300):
                rows.append([file_no_md, hdgs, chunk, count_tokens(chunk)])
        else:
            rows.append([file_no_md, hdgs, para, count_tokens(para)])

    df = pd.DataFrame(rows, columns=["file", "heading", "content", "tokens"])
    df = df[df.tokens > 40].reset_index(drop=True)
    df_f = pd.concat([df_f, df]).reset_index(drop=True)

df_f

df_f.to_pickle("dataframe.pickle")

#pd.read_pickle("dataframe.pickle")
#pd.read_pickle("dataframe.pkl")
#pd.read_arrow("dataframetable.arrow")


def get_embedding(text: str, model: str=EMBEDDING_MODEL):
    result = openai.Embedding.create(
      model=model,
      input=text
    )
    return result["data"][0]["embedding"]


def compute_doc_embeddings(df: pd.DataFrame):
    """
    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.

    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.
    """
    return {
        idx: get_embedding(r.content) for idx, r in df.iterrows()
    }

vector_embedding = compute_doc_embeddings(df_f)

df_f['vector_embedding'] = pd.Series(vector_embedding)
df_f.head()

from openai.embeddings_utils import cosine_similarity

def order_documents_query_similarity(data, query_str, nres=3):
    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)
    data['similarities'] = data.vector_embedding.apply(lambda x: cosine_similarity(x, embedding))

    res = data.sort_values('similarities', ascending=False).head(nres)
    return res

res = order_documents_query_similarity(df_f, "What is the first step to creating a tapis application?")
res.head()

def construct_prompt(question: str, df: pd.DataFrame, ncontents = 3) -> str:
    """
    Fetch relevant
    """
    most_relevant_document_sections = order_documents_query_similarity(df, question)

    chosen_sections = []
    chosen_section_len = 0

    MAX_SECTION_LEN = 500
    context = order_documents_query_similarity(df, question)
    context.head()

    for _, ctx in context.iterrows():
        chosen_section_len += ctx.tokens
        if chosen_section_len > MAX_SECTION_LEN:
            break

        chosen_sections.append(" " + ctx.content.replace("\n", " "))

    header = """Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say "I don't know."\n\nContext:\n"""

    return header + "".join(chosen_sections) + "\n\n Q: " + question + "\n A:"

construct_prompt(question="What is the first step to creating a tapis application?", df=df_f)

COMPLETIONS_API_PARAMS = {
    "temperature": 0.0,
    "max_tokens": 300,
    "model": COMPLETIONS_MODEL,
}

def answer_query_with_context(
    query: str,
    df: pd.DataFrame,
    show_prompt: bool = False) -> str:

    prompt = construct_prompt(
        query,
        df
    )

    if show_prompt:
        print(prompt)

    response = openai.Completion.create(
                prompt=prompt,
                **COMPLETIONS_API_PARAMS
            )

    return response["choices"][0]["text"].strip(" \n")

answer_query_with_context("What is the first step to creating a tapis application?", df_f)

answer_query_with_context("What is the meaning of life?", df_f)
