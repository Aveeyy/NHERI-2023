# -*- coding: utf-8 -*-
"""Avey Copy of Branch_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiRmSupkD2C0lhRcl4_yT1bGOLCiuO84
"""

!pip install openai
!pip install markdown2
!pip install transformers

COMPLETIONS_MODEL = "text-davinci-003"
EMBEDDING_MODEL = "text-embedding-ada-002"

import numpy as np
import openai
import os
import pandas as pd
import pickle
import pyarrow

import os
openai.api_key = os.getenv("OPENAI_API_KEY")
assert openai.api_key, "Missing OPENAI_API_KEY env var"

from google.colab import drive
drive.mount('/content/drive')

directory = "/content/drive/MyDrive/Technical Documentation "
for i in os.listdir(directory):
  print(i)

import markdown2
from bs4 import BeautifulSoup
from transformers import GPT2TokenizerFast

import numpy as np
from nltk.tokenize import sent_tokenize

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

data = []

#creating empty data frame
#df_f = pd.DataFrame()
df_f = pd.DataFrame(data, columns=["file", "heading", "content", "tokens"])
#df_f = df_final.reset_index().drop('index',axis=1) # reset index

#iterating through each file to tokenize
for i in os.listdir(directory):
      with open(f"{i}", "r") as file:
          content = file.read()
      file_no_md = i.replace(".md", "")
      file_no_md = "tapipy/" + file_no_md

      # Use markdown2 to convert the markdown file to html
      html = markdown2.markdown(content)

      # Use BeautifulSoup to parse the html
      soup = BeautifulSoup(html, "html.parser")

      # Initialize variables to store heading, subheading, and corresponding paragraphs
      headings = []
      paragraphs = []
      data = []
      MAX_WORDS = 500

      def count_tokens(text: str) -> int:
          """count the number of tokens in a string"""
          return len(tokenizer.encode(text))

      def break_text(text, max_tokens):
        tokens = text.split()
        chunks = []
        current_chunk = ""

        for token in tokens:
            if len(current_chunk) + len(token) < max_tokens:
                current_chunk += token + " "
            else:
                chunks.append(current_chunk.strip())
                current_chunk = token + " "

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks



      # Iterate through the tags in the soup
      for tag in soup.descendants:
          # Check if the tag is a heading
          if tag.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
              # When the next heading is encountered, print the heading, subheading, and corresponding paragraphs
              if headings and paragraphs:
                  hdgs = " ".join(headings)
                  para = " ".join(paragraphs)
                  counting = count_tokens(para)
                  if counting > 1024:
                    para_chunks = break_text(para, 1024)
                    for chunk in para_chunks:
                      data.append([file_no_md, hdgs, chunk, count_tokens(chunk)])
                      headings = []
                      paragraphs = []
                  else:
                    data.append([file_no_md, hdgs, para, count_tokens(para)])
                    headings = []
                    paragraphs = []
              # Add to heading
              headings.append(tag.text)
          # Check if the tag is a paragraph
          elif tag.name == "p":
              paragraphs.append(tag.text)

      df = pd.DataFrame(data, columns=["file", "heading", "content", "tokens"])
      df = df[df.tokens>40]
      df = df.reset_index().drop('index',axis=1) # reset index
      df_f = pd.concat([df_f, df])
      df_f = df_f.reset_index().drop('index',axis=1)

df_f

df_f.to_pickle("dataframe.pickle")

#pd.read_pickle("dataframe.pickle")
#pd.read_pickle("dataframe.pkl")
#pd.read_arrow("dataframetable.arrow")


def get_embedding(text: str, model: str=EMBEDDING_MODEL):
    result = openai.Embedding.create(
      model=model,
      input=text
    )
    return result["data"][0]["embedding"]


def compute_doc_embeddings(df: pd.DataFrame):
    """
    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.

    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.
    """
    return {
        idx: get_embedding(r.content) for idx, r in df.iterrows()
    }

vector_embedding = compute_doc_embeddings(df_f)

df_f['vector_embedding'] = pd.Series(vector_embedding)
df.head()

from openai.embeddings_utils import cosine_similarity

def order_documents_query_similarity(data, query_str, nres=3):
    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)
    data['similarities'] = data.vector_embedding.apply(lambda x: cosine_similarity(x, embedding))

    res = data.sort_values('similarities', ascending=False).head(nres)
    return res

res = order_documents_query_similarity(df, "What is the first step to creating a tapis application?")
res.head()

def construct_prompt(question: str, df: pd.DataFrame, ncontents = 3) -> str:
    """
    Fetch relevant
    """
    most_relevant_document_sections = order_documents_query_similarity(df, question)

    chosen_sections = []
    chosen_section_len = 0

    MAX_SECTION_LEN = 500
    context = order_documents_query_similarity(df, question)
    context.head()

    for _, ctx in context.iterrows():
        chosen_section_len += ctx.tokens
        if chosen_section_len > MAX_SECTION_LEN:
            break

        chosen_sections.append(" " + ctx.content.replace("\n", " "))

    header = """Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say "I don't know."\n\nContext:\n"""

    return header + "".join(chosen_sections) + "\n\n Q: " + question + "\n A:"

construct_prompt(question="What is the first step to creating a tapis application?", df=df)

COMPLETIONS_API_PARAMS = {
    "temperature": 0.0,
    "max_tokens": 300,
    "model": COMPLETIONS_MODEL,
}

def answer_query_with_context(
    query: str,
    df: pd.DataFrame,
    show_prompt: bool = False) -> str:

    prompt = construct_prompt(
        query,
        df
    )

    if show_prompt:
        print(prompt)

    response = openai.Completion.create(
                prompt=prompt,
                **COMPLETIONS_API_PARAMS
            )

    return response["choices"][0]["text"].strip(" \n")

answer_query_with_context("What is the first step to creating a tapis application?", df)

answer_query_with_context("What is the meaning of life?", df)
